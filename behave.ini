# Behave Configuration for Torua E2E Testing

[behave]
# Test execution settings
default_format = pretty
show_skipped = true
show_timings = true
summary = true
expand = true
color = true

# Output settings
stdout_capture = no
stderr_capture = no
log_capture = yes
logging_level = INFO
logging_format = %(asctime)s [%(levelname)s] %(name)s: %(message)s

# Test discovery
paths = features/
recursive = true

# Reporting
junit = true
junit_directory = test-results/

# Tags to exclude by default (can be overridden with -t)
# Use @wip for work-in-progress scenarios
# Use @slow for performance tests
# Use @integration for integration tests
default_tags = -@wip -@skip

# Step definitions location
steps = features/steps

# Environment setup
# environment.py will handle setup/teardown

# Parallel execution settings (when using behave-parallel)
# parallel_processes = 4
# parallel_scheme = scenario

# Stop on first failure (useful during development)
# stop = true

# Dry run (just verify steps are defined)
# dry_run = true

# Show undefined steps
show_undefined = true

# Show source location of steps
show_source = true

# Language settings
lang = en

# Scenario outline settings
scenario_outline_annotation_schema = {name} -- @{row.id} {examples.name}

[behave.userdata]
# Custom configuration for our tests
coordinator_host = localhost
coordinator_port = 8080
node1_port = 8081
node2_port = 8082
node3_port = 8083
startup_timeout = 10
request_timeout = 5
cleanup_on_exit = true
log_level = INFO
test_data_dir = features/test_data
concurrent_clients = 10
performance_threshold_ms = 50
large_value_size_mb = 1
